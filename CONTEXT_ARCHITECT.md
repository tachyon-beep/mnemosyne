# Context Management System Architect

## Overview

This document serves as the system architect for implementing Phase 2: Intelligent Context Management with a focus on flexible summarization supporting both local and external LLM providers.

## Implementation Strategy

### Phase 1: Core Infrastructure (Current)

1. **Database Schema**
   - Create migration 003_intelligent_context.ts
   - Add tables: conversation_summaries, summary_cache, llm_providers, summary_history
   - Set up proper indexes and foreign keys

2. **Provider Interface**
   - Define abstract LLMProvider interface
   - Create ProviderConfig and ProviderFactory
   - Implement provider health checking

3. **Token Management**
   - Implement accurate token counters for different models
   - Create TokenBudgetManager
   - Add chunking strategies for large texts

### Phase 2: Provider Implementations

1. **Local Provider (Ollama)**
   ```typescript
   // src/context/providers/OllamaProvider.ts
   - HTTP client for Ollama API
   - Model management and availability checking
   - Streaming response handling
   - Error handling and retries
   ```

2. **External Provider (OpenAI)**
   ```typescript
   // src/context/providers/OpenAIProvider.ts
   - OpenAI API integration
   - Cost tracking and estimation
   - Rate limiting and quota management
   - API key validation
   ```

3. **Provider Manager**
   ```typescript
   // src/context/ProviderManager.ts
   - Provider registration and lifecycle
   - Fallback chain management
   - Provider health monitoring
   - Cost aggregation
   ```

### Phase 3: Summarization Engine

1. **Summary Generator**
   ```typescript
   // src/context/SummaryGenerator.ts
   - Hierarchical summary generation
   - Template-based prompts for consistency
   - Quality validation and scoring
   - Batch processing support
   ```

2. **Summary Storage**
   ```typescript
   // src/context/SummaryRepository.ts
   - CRUD operations for summaries
   - Version management
   - Cache integration
   - Expiration handling
   ```

### Phase 4: Context Assembly

1. **Context Assembler**
   ```typescript
   // src/context/ContextAssembler.ts
   - Relevance scoring algorithms
   - Token budget optimization
   - Mixed content strategies
   - Progressive detail support
   ```

2. **Cache Layer**
   ```typescript
   // src/context/ContextCache.ts
   - LRU cache implementation
   - TTL-based expiration
   - Memory management
   - Persistent cache support
   ```

### Phase 5: MCP Tools

1. **New Tools**
   - get_context_summary
   - get_relevant_snippets
   - configure_llm_provider
   - get_summary_status

2. **Tool Integration**
   - Register with ToolRegistry
   - Add to tool exports
   - Update TypeScript types

## Key Design Decisions

### 1. Provider Abstraction
- **Decision**: Abstract provider interface with factory pattern
- **Rationale**: Allows easy addition of new providers without changing core logic
- **Trade-offs**: Slight complexity increase vs flexibility gain

### 2. Local-First with External Fallback
- **Decision**: Default to local providers, fall back to external
- **Rationale**: Privacy-preserving while ensuring reliability
- **Trade-offs**: Local models may have lower quality but preserve privacy

### 3. Hierarchical Summarization
- **Decision**: Three-level hierarchy (brief/standard/detailed)
- **Rationale**: Balances information density with token constraints
- **Trade-offs**: More storage vs better user experience

### 4. Async Summary Generation
- **Decision**: Background processing with status tracking
- **Rationale**: Prevents blocking on slow LLM inference
- **Trade-offs**: Complexity of async state management

## Technical Architecture

### Component Hierarchy
```
src/
â”œâ”€â”€ context/
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”œâ”€â”€ LLMProvider.ts          # Abstract interface
â”‚   â”‚   â”œâ”€â”€ OllamaProvider.ts       # Local Ollama implementation
â”‚   â”‚   â”œâ”€â”€ OpenAIProvider.ts       # OpenAI API implementation
â”‚   â”‚   â””â”€â”€ MockProvider.ts         # Testing provider
â”‚   â”œâ”€â”€ SummaryGenerator.ts         # Core summarization logic
â”‚   â”œâ”€â”€ ContextAssembler.ts         # Context optimization
â”‚   â”œâ”€â”€ TokenCounter.ts             # Token counting utilities
â”‚   â”œâ”€â”€ ProviderManager.ts          # Provider lifecycle
â”‚   â”œâ”€â”€ SummaryRepository.ts        # Database operations
â”‚   â””â”€â”€ ContextCache.ts             # Caching layer
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ GetContextSummaryTool.ts    # Summary retrieval
â”‚   â”œâ”€â”€ GetRelevantSnippetsTool.ts  # Snippet extraction
â”‚   â””â”€â”€ ConfigureLLMProviderTool.ts # Provider configuration
â””â”€â”€ storage/
    â””â”€â”€ migrations/
        â””â”€â”€ 003_intelligent_context.ts
```

### Data Flow
```
1. User Request â†’ MCP Tool
2. Tool â†’ Context Assembler
3. Context Assembler â†’ Summary Generator (if needed)
4. Summary Generator â†’ Provider Manager
5. Provider Manager â†’ Selected Provider (Local/External)
6. Provider â†’ LLM â†’ Summary
7. Summary â†’ Storage & Cache
8. Assembled Context â†’ User
```

## Implementation Priorities

### Week 1: Foundation
1. âœ… Create HLD-Context.md
2. â¬œ Implement database migration
3. â¬œ Create provider interface and base classes
4. â¬œ Implement token counting utilities

### Week 2: Providers
1. â¬œ Implement Ollama provider
2. â¬œ Implement OpenAI provider
3. â¬œ Create provider manager
4. â¬œ Add provider configuration

### Week 3: Summarization
1. â¬œ Build summary generator
2. â¬œ Implement summary storage
3. â¬œ Add quality validation
4. â¬œ Create summary templates

### Week 4: Context Assembly
1. â¬œ Build context assembler
2. â¬œ Implement token optimization
3. â¬œ Add caching layer
4. â¬œ Progressive detail support

### Week 5: Integration
1. â¬œ Create MCP tools
2. â¬œ Integration testing
3. â¬œ Performance optimization
4. â¬œ Documentation

## Testing Strategy

### Unit Tests
- Provider implementations
- Token counting accuracy
- Summary quality validation
- Cache operations

### Integration Tests
- Provider failover scenarios
- Summary generation pipeline
- Context assembly optimization
- MCP tool functionality

### Performance Tests
- Summary generation latency
- Token counting speed
- Cache effectiveness
- Memory usage

## Configuration Examples

### Local-Only Setup
```json
{
  "providers": {
    "default": "ollama",
    "ollama": {
      "endpoint": "http://localhost:11434",
      "model": "llama2:7b",
      "timeout": 30000
    }
  }
}
```

### Hybrid Setup
```json
{
  "providers": {
    "default": "ollama",
    "fallback": "openai",
    "ollama": {
      "endpoint": "http://localhost:11434",
      "model": "mistral:latest"
    },
    "openai": {
      "model": "gpt-3.5-turbo",
      "temperature": 0.7,
      "maxTokens": 1000
    }
  }
}
```

## Security Considerations

1. **API Key Management**
   - Environment variables only
   - Never log sensitive data
   - Validate before use

2. **Data Minimization**
   - Strip PII before external calls
   - Implement content filtering
   - User consent for external providers

3. **Cost Protection**
   - Implement spending limits
   - Track usage per user
   - Alert on unusual activity

## Success Metrics

1. **Quality Metrics**
   - Summary coherence score > 0.8
   - Information retention > 90%
   - User satisfaction > 4.5/5

2. **Performance Metrics**
   - Local summary < 5 seconds
   - External summary < 10 seconds
   - Context assembly < 500ms

3. **Reliability Metrics**
   - Provider availability > 99%
   - Fallback success rate > 95%
   - Cache hit rate > 60%

## Next Steps

1. Create database migration file
2. Implement provider interface
3. Build Ollama provider first (local-first approach)
4. Set up testing infrastructure

Ready to start implementation! ðŸš€